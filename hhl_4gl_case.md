# Языки более высокого уровня, языки 4-ого поколения и CASE

В Прологе я упомянул концепцию компиляторов компиляторов. В то время, когда они меня больше всего интересовали (середина 60-х), общепринятым было мнение, что более сложные компиляторы были ответом на проблему производительности. Всем было ясно, что выражение вроде:

`W = (X + Y) / (Z + 3)`

Бесконечно превосходит эквивалент машинного языка, который мог бы выглядеть примерно так:

```
LOAD  Z
ADD   3
STORE TEMP
LOAD  X
ADD   Y
DIV   TEMP
STORE W
```

Это пример машины с одним аккумулятором, но суть вы поняли. Одна из причин, по которой синтаксис, показанный в первой строке, может быть эффективным мостом между людьми и компьютерами, в том, что он синтаксически чист и основывался на прочной, хорошо понятной математической основе, а именно арифметике... За исключением довольно странное использование знака равенства!

Разумно было ожидать, что эту выразительность можно распространить и на другие функции, которые машины должны выполнять. Хоть ввод-вывод становился все более сложным на уровне машинного языка, операционные системы продолжали прогрессировать, что все же позволяло программисту написать один оператор для выполнения простой операции ввода-вывода. На IBM 1401 команда чтения карты состоит из одной инструкции и одного символа! В то время как GET от MVS мог вызвать выполнение нескольких сотен и тысяч инструкций на машинном языке, а программист по-прежнему писал один оператор. 

На этой основе мы начали создавать один язык программирования за другим: COBOL должен был стать языком, который позволил бы людям с улицы или, по крайней мере, менеджерам заниматься программированием! Алгол стал стандартом документации для алгоритмов. IBM разработала PL/I (я работал над одной довольно недолговечной версией); люди разрабатывали компиляторы в своих подвалах; аспиранты писали компиляторы для своих диссертаций  (и сейчас пишут). Всегда было ощущение, что один из этих языков станет ключом к разблокировке великой производительности. Хоть наука о построении компиляторов продвигалась семимильными шагами, в целом продуктивность программистов не слишком выросла.

COBOL и PL / I были компиляторами общего назначения. Также было много языков, специализирующихся на определенных задачах: языки моделирования, языки с паттерн-матчингом, языки генерации отчетов. И давайте не забывать APL - APL - чрезвычайно мощный язык, он открыл такие загадочные области математики, как обработка матриц, для тех из нас, кто никогда не понимал её в школе. Умение умножать матрицу за 5 нажатий клавиш `(A + .xB)` по-прежнему остается уровнем выразительности, с которым, как мне кажется, немногие языки программирования когда-либо смогут сравниться! Сочетание огромной мощности в математической области и отсутствия типизации во время компиляции позволяло создавать и запускать интересные программы чрезвычайно быстро. Однажды я прочитал комментарий в математической статье о том, что автор не думал, что работа была бы возможна без APL - и я ему верю. Хотя он в определенной мере использовался в бизнесе для программирования, ориентированного на вычисления, особенно в банковском деле и страховании, а также в качестве основы для некоторых из ранних языков запросов, APL мало что сделал для большинства коммерческих разработок, продолжал использоваться COBOL и PL / I, PASCAL, BASIC...

APL по-своему показывает важность минимизации «разрыва» между идеей и ее воплощением - наряду со всеми наиболее популярными языками, это интерпретатор, что означает, что вы можете ввести программу, а затем сразу же запустить ее. Без необходимости выполнять этап компиляции или компоновки. Конечно, это больше восприятие, чем реальный факт (поскольку можно создавать шаги компиляции, которые настолько быстры, что пользователь не воспринимает их как препятствие (прим.пер. - привет, Go)), но факт остается фактом: некоторые очень неудобные языки стали чрезвычайно популярными, потому как не требуется этап компиляции. Многие циклы CPU на машинах IBM, используются для выполнения CMS EXEC или TSO CLIST. Оба они являются простыми языками, которые позволяют объединять команды в исполняемые «программы». В настоящее время оба уступают место REXX Майка Коулишоу, который занимает ту же нишу, но также предоставляет гораздо более мощный набор языковых конструкций до такой степени, что с его помощью можно создавать довольно сложные программы. REXX также является интерпретируемым, поэтому он также позволяет изменить программу и очень быстро увидеть результат.

Почему языки (даже интерпретируемые) не увеличили продуктивность больше, чем смогли? Я исследую этот вопрос подробней в следующей главе, но одной вещь, что я заметил довольно рано, было то, что все они мало что сделали для логики (`IF, THEN, ELSE, DO WHILE` и т. Д.). Для многих видов бизнес-программирования время разработки увеличивается из-за логики - на самом деле прямых вычислений может быть и немного. Логический выбор можно рассматривать как определенный объем работы, независимо от того, напишете ли вы его так:

```
IF x > 2
THEN
result = 1
ELSE
result = 2
ENDIF
```

или так:

```
result = (x>2) ? 1 : 2;
```

Можно хоть нарисовать как диаграмму Наси-Шнейдермана или Чапина. Можно утверждать, что, поскольку обе приведенные выше фразы предполагают одно бинарное решение, они требуют примерно одинакового объема умственной работы. Чем сложнее логика, тем сложнее кодирование. Фактически, в отрасли довольно широко используется мера сложности, называемая цикломатической сложностью МакКейба, которая напрямую основана на количестве бинарных решений в программе. Однако в своей работе мы обнаружили, что количество логики в обычном программировании можно уменьшить, потому что большая часть логики в обычном программировании связана с синхронизацией данных, а не с бизнес-логикой. Поскольку FBP устраняет необходимость в большой части этой логики синхронизации, это означает, что FBP сокращает количество логики в программах.

Ряд авторов отметили, что продуктивность улучшается только в том случае, если вы можете уменьшить количество утверждений, необходимых для представления идеи. Другими словами, вы должны сократить «разрыв» между языком бизнеса и языком компьютера. Но каков же минимум битов или нажатий клавиш для представления идеи? Если условие и его ветви составляют одну «идею», то существует нижний предел того, насколько компактно оно может быть представлено. Если это часть большей «идеи», то есть надежда представить ее более компактно. Из теории информации мы узнаем, что количество битов, необходимых для представления чего-либо, - это квадратный логарифм количества альтернатив, которые можно выбрать. Если что-то всегда верно, выбора нет, поэтому для представления не нужны никакие биты. Если у вас есть 8 вариантов, то для его представления требуется 3 бита (т.е. логарифм из 8 по основанию 2). С точки зрения программирования: если вы разрешаете человеку только два брачных состояния, вам нужен только 1 бит (логарифм от 2 до основания 2). Если вы хотите поддержать более широкую «вселенную», где люди могут быть одинокими, женатыми, разведенными, овдовевшими или гражданскими супругами, это пять альтернатив, поэтому вам нужно 3 бита (2 бита недостаточно, поскольку они позволяют только 4 варианта выбора. ). И они не исключают друг друга, поэтому вам может понадобиться еще больше битов!

Это, в свою очередь, приводит к следующей идее: один из способов уменьшить информационную нагрузку программы - выбрать варианты из более ограниченной вселенной. Однако пользователь должен быть готов жить в этой более ограниченной вселенной. Я помню пакет бухучета, разработанный в Западной Канаде, который очень четко определял вселенную, в которой должны были работать ее клиенты. В этой вселенной он выполнял довольно много функций. Его основные файловые записи всегда имели длину 139 байт, и вы не могли их изменить. Если бы вы спросили их об этом, разработчики ответили бы: зачем кому-то это нужно? Как и ожидалось, он не прижился, потому что многие клиенты считали его слишком ограниченным. Мне запомнился пример одного клиента, который хотел изменить заголовок отчета, и ему сказали, что это невозможно. Опять же, почему кто-то мог подумать, что это так важно? Что ж, похоже, клиенты, особенно крупные, склонны считать, что заголовки отчетов должны выглядеть так, как они хотят. И это действительно был достаточно мощный пакет за свою цену! Я многому научился на этом, и главное, что поставщик может предоставить стандартные компоненты, но заказчик также должен иметь возможность писать собственные. Даже если последнее стоит дороже, это выбор клиента, а не продавца. И это, конечно, означает, что клиент должен иметь возможность визуализировать, что делает инструмент. Это также связано с принципом открытой архитектуры: каким бы впечатляющим ни был инструмент, если он не может взаимодействовать с другими инструментами, он не сможет выжить (перефразировано из Уэйна Стивенса).

The above information-theoretic concept is at the root of what are now called 4GLs (4th Generation Languages). These provide more productivity by taking advantage of frequently appearing application patterns, e.g. interactive applications. If you are writing applications to run in an interactive system, you know that you are going to keep running into patterns like:

    read a key entered by the user onto a screen

    get the correct record, or generate a message if the record does not exist

    display selected fields from that record on the screen.

Another one (very often the next one) might be:

    display the fields of a record

    determine which ones were changed by the user

    check for incorrect formats, values, etc.

    if everything is OK,
    write the updated record back

    else
    display the appropriate error message(s)

Another very common pattern (especially in what is called "decision support" or "decision assist" type applications) occurs when a list is presented on the screen and the user can select one of the items or scroll up or down through the list (the list may not all fit on one screen). Some systems allow more than one item to be selected, which are then processed in sequence.

These recurrent patterns occur so frequently that it makes a lot of sense to provide skeletons for these different scenarios, and declarative (non-procedural) ways of having the programmer fill in the information required to flesh them out.

The attractiveness of 4GLs has also been enhanced by the unattractiveness of IBM's standard screen definition facilities! The screen definition languages for both IMS and CICS are coded up using S/370 Assembler macros (high-level statements which generate the constants which define all the parts of a screen). This technique allows them to provide a lot of useful capabilities, but screen definitions written this way are hard to write and even harder to maintain! Say you want to make a field longer and move it down a few lines, you find yourself changing a large number of different values which all have to be kept consistent (the values are often not even the same, but have to be kept consistent according to some formula). I once wrote a prototyping tool which allowed screens to be specified in WYSIWYG (What You See Is What You Get) format, and could then be used to generate both the screen definition macros and also all the HLL declares that had to correspond to it. It was quite widely used internally within IBM, and in fact one project, which needed to change some MFS, started out by converting the old MFS into the prototyper specifications, so that they could make their changes, and then generate everything automatically. This way, they could be sure that everything stayed consistent. When such a screen definition tool is integrated with a 4GL, you get a very attractive combination. It's even better when the prototyping tool is built using FBP, as it can then be "grown" into a full interactive application by incrementally expanding the network. This ability to grow an application from a prototype seems very desirable, and is one of the things that make FBP attractive for developing interactive applications.

The problem, of course, with the conventional 4GL comes in when the customer, like our customer above who wanted to change a report title, wants something that the 4GL does not provide. Usually this kind of thing is handled by means of exits. A system which started out simple eventually becomes studded with exits, which require complex parametrization, and whose function cannot be understood without understanding the internals of the product - the flow of the product. Since part of the effectiveness of a 4GL comes from its being relatively opaque and "black boxy", exits undermine its very reason for being.

An example of this in the small is IBM's MVS Sort utility (or other Sorts which are compatible with it) - as long as one can work with the standard parameters for the Sort as a whole, it's pretty clear what it is doing, and how to talk to it. Now you decide you want to do some processing on each input record before it goes into the Sort. You now have to start working with the E15 exit. This requires that you form a concept of how Sort works on the inside - a very different matter. E15 and E35 (the output exit routine) have to be independent, non-reentrant, load modules, so this puts significant constraints on the ways in which applications can use load module libraries... and so on. Luckily Sort also has a LINKable interface, so DFDM [and AMPS before it] used this, turned E15 and E35 inside-out, and converted the whole thing into a well-behaved reusable component. Much easier to use and you get improved performance as well due to the reduction in I/O! In a similar sort of way, FBP can also capitalize on the same regularities as 4GLs do by providing reusable components (composite or elementary) as well as standard network shapes. Instead of programmers having to understand the internal logic of a 4GL, they can be provided with a network and specifications of the data requirements of the various components. Instead of having to change mental models to understand and use exits, the programmer has a single model based on data and its transformations, and is free to rearrange the network, replace components by custom ones, or make other desired changes.

I should point out also that the regularities referred to above have also provided a fertile breeding-ground for various source code reuse schemes. My feeling about source code reuse is that it suffers from a fundamental flaw: even if building a new program can be made relatively fast, once you have built it, it must be added to the ever-growing list of programs you have to maintain. It is even worse if, as is often required, the reusable source code components have to be modified before your program can work, because then you have lost the trail connecting the original pieces of code to your final program if one of them has to be enhanced, e.g. to fix a bug. Even if no modification takes place, the new program has to be added to the list of program assets your installation owns. Already in some shops, maintenance is taking 80% of the programming resource, so each additional application adds to this burden. In FBP, ideally all that is added to the asset base is a network - the components are all black boxes, and so a new application costs a lot less to maintain.

A related type of tool are program generators - this is also source-level reuse with a slightly different emphasis. As above, an important question is whether you can modify the generated code. If you can't, you are limited to the choices built into the generator; if you can, your original source material becomes useless from a maintenance point of view, and can only be regarded as a high-level (and perhaps even misleading) specification. Like out of date documentation, it might almost be safer to throw it away...

I don't want to leave this general area without talking about CASE (Computer-Aided Software Engineering) tools. The popularity of these tools arises from several very valid concepts. First, people should not have to enter the same information multiple times - especially when the different forms of this data are clearly related, but you have to be a computer to figure out how! We saw this in the case of the prototyping tool mentioned above. If you view the development process as a process of expressing creativity in progressive stages within the context of application knowledge, then you want to capture this in a machine, and not just as text, but in a form which captures meaning, so that it can be converted to the various formats required by other software, on demand. This information can then be converted to other forms, added to, presented in a variety of display formats, etc.

There are a number of such tools out in the marketplace today, addressing different parts of the development process, and I see these as the forerunners of the more sophisticated tools which will become available in the next few years. Graphical tools are now becoming economical, and I believe that graphical techniques are the right direction, as they take advantage of human visualization skills. I happen to believe HIPOs (remember them? Hierarchical Input, Process, Output) had part of the right answer, but adequate graphical tools were not available in those days, and maintaining them with pencil, eraser and a template was a pain! However, when someone went through all that trouble, and produced a finished HIPO diagram, the results were beautiful and easy to understand. Unfortunately, systems don't stand still and they were very hard to maintain, given the technology of those days!

Our experience is that Structured Analysis is a very natural first stage for FBP development, so CASE tools which support Structured Analysis diagrams and which have open architectures are natural partners with FBP. In fact, FBP is the only approach which lets you carry a Structured Analysis design all the way to execution - with conventional programming, you cannot convert the design into an executable program structure. There is a chasm, which nobody has been able to bridge in practice, although there are some theoretical approaches, such as the Jackson Inversion, which have been partially successful. In FBP, you can just keep adding information to a design which uses the Structured Analysis approach until you have a working program. In what follows, you will see that FBP diagrams do not really require much information at the network level to create a running program which is not already captured by the Structured Analysis design. Probably the most important point is that one has to distinguish between code components and processes (occurrences of components), and some Structured Analysis tools do not make a clear distinction between these two concepts. As we shall see in the following chapter, an FBP network consists of multiple communicating processes, but a tool which is viewed as primarily being for diagramming may be forgiven for assuming that all the blocks in a diagram are unique, different programs. The need to execute a picture imposes a discipline on the design process and the designer, which means that these confusions have to be resolved. We actually developed some PC code to convert a diagram on one of the popular CASE tools into a DFDM network specification, which was used successfully for several projects.

FBP's orientation towards reuse also forces one to distinguish between a particular use of a component and its general definition. This may seem obvious in hindsight, but, even when documenting conventional programs, you would be amazed how often programmers give a fine generalized description of, say, a date routine, but forget to tell the reader which of its functions is being used in a particular situation. Even in a block diagram I find that programmers often write in the general description of the routine and omit its specific use (you need both). This is probably due to the fact that, in conventional programming, the developer of a routine is usually its only user as well, so s/he forgets to "change hats". When the developer and user are different people, it is easier for the user to stay in character.

To summarize, HLLs, 4GLs and CASE are all steps along a route towards the place we want to be, and all have lessons to teach us, and capabilities which are definitely part of the answer. What is so exciting about FBP is that it allows one to take the best qualities of all the approaches which went before, and combine them into a larger whole which is greater than the sum of its parts.
