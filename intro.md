# Введение

Представьте, у вас онлайн-магазин и вы поняли что надо в сжатые сроки внести серьёзные изменения. Программисты говорят, правки займут несколько месяцев, но они ещё посмотрят. Назначается встреча, туда зовут всех, не только разработчиков с аналитиками, но также рабочий персонал и пользователей. Структура программы отображается на стене, и дизайнеры системы, собравшись в группу, начинают её исследовать. Внезапно выясняется, что нужно всего-то набросать пару новых компонентов, да переподключить чуть иначе парочку старых. Новый прогноз на выполнение задачи - неделя!

FBP не было популярно, вероятно, из-за сдвига парадигмы, что требует от мышления, но сейчас для него самое время. Оно даёт последовательный взгляд на приложение от орлиного взора, вглубь, до реализации низкоуровневых модулей. Но требует писать приложение как переиспользуемые "чёрные ящики". Это заставляет программистов фокусироваться на данных и их трансформациях. Это быстрое прототипирование и надёжный результат. Это совместимость с распределёнными системами и пересечение с ООП.

Слишком хорошо, чтобы быть правдой? Судите сами! Что будет описано далее, я считаю настоящей революцией в создании программ и в поддержке
требований к обработке данных от компаний по всему миру.

Большинство приложений написано на т.н. "языках уровнем повыше" (Higher level languages).
Есть также языки "пятого поколения", ещё более высокоуровневые, чем HHL, но более специализированные.
Несмотря на богатство инструментария, программист сегодня должен преобразовать свои элегантные идеи в строки команд на конкретном языке. Генераторы имели определённый успех, но не решили проблему - большинство программ до сих пор пишутся вручную. Программисты до сих пор будто мастера-мебельщики, вручную выстругивающие стулья из красного дерева.

И пару слов о т.н. "процедурных" или "организационных" подходы к улучшению процесса разработки вроде "structured walk-throughs", "the buddy system", "chief programmer teams", "third-party testing". Они хороши и будут давать результат независимо от парадигмы.

Без разработчиков приложений бизнес во всём мире встанет. Эти люди практикуют ремесло, которого большинство не понимает. Архитипичный программист это одарённый, но непрактичный индивид который лучше сходится с машинами, чем с людьми. На самом деле, конечно, программист это интерфейс между его клиентами, говорящими на языке бизнеса, и их системами, говорящими на языке электронов. И чем эффективнее программист может заполнить пропасть между этими мирами,
тем лучше будет приложение, что он пишет. Однако, всё это требует от него множества навыков.

Проблемы современного инструментария программирования рождаются чуть ли не полностью от несоответствия между областью проблемы, в которой программист работает и тулчейном, которым он или она вынужден пользоваться. Лишь сократив пропасть между миром пользователей и разработчиков, мы сможем поставлять приложения удовлетворяющие всем нуждам и делать это эффективно с точки зрения финансов время-затрат.

Важная вещь о разработке приложений, что я понял за последние 20 лет - сам процесс толком не изменился.
Дело тут, конечно, не в недостатке адептов какого-то нового блестящего инструмента. Тем не менее, очень немногие из этих
инструментов дали то, что обещали. Когда я начал свою карьеру в 1959 году, у нас уже были высокоуровневые языки, интерпретаторы и вызовы подпрограмм - это все еще основные инструменты современных профессионалов в области программирования. Тип программирования, которым занимается большинство из нас, уходит корнями в процедурное программирование, возникшее в 40-50-х годах. Тогда новое изобретение, называемое компьютером, удовлетворило растущую потребность в повторяющихся математических вычислениях, таких как таблицы приливов и отливов, баллистические расчеты и расчеты переписи. В этих областях компьютеры пользовались огромным успехом.
Однако даже тогда некоторые эксперты в этом новом мире вычислений начали сомневаться, действительно ли процедурное программирование приложений подходит для создания бизнес-приложений. Комбинация все более и более сложных систем, явная сложность работы программистов среднего уровня и необходимость сокращения накладных расходов предприятиями приводит к все большему давлению на сегодняшних профессионалов в области программирования.

Кроме того, по мере того, как программисты создают новые системы, растёт количество ресурсов, расходуемых на их поддержку, до такой степени, что на способность многих компаний разрабатывать новые приложения серьезно влияет бремя поддержки старых. Это, в свою очередь, отрицательно сказывается на их способности конкурировать на новом конкурентном глобальном рынке.

Часто упоминается технический долг - невыполненные задачи по программированию, которые планируется выполнить, но которые нельзя взять в работу из-за нехватки ресурсов. Я также слышал, как люди используют фразу «скрытый техдолг» - это работа, которую пользователи хотели бы видеть, но всем ясно, что нет смысла даже говорить о ней, поэтому она, как правило, не отображается в статистике! Я думаю, это, по крайней мере, частично, причина, почему отделы, не связанные с вычислениями, в последние годы скупают ПК - они думают, что наличие собственных компьютеров сделает их независимыми, но, конечно, они просто столкнутся с теми же старыми проблемами. На собственных машинах!

Когда-то предсказывалось, что потребуется больше операторов телефонных коммутаторов, чем доступных молодых дамочек. Эта проблема была решена разработкой систем автоматической телефонной коммутации. Точно так же многие теперь думают, что нынешнюю ситуацию в вычислительной технике можно решить только с помощью квантового скачка в технологиях, и, конечно же, каждая новая программная технология претендует на то, чтобы стать долгожданным решением. Я и ряд других людей полагаем, что концепции, описанные ниже, и есть решение, и я надеюсь, что, читая эту книгу, вы согласитесь с нами. Однако они представляют собой настоящую смену парадигмы, которая коренным образом меняет то, как мы смотрим на процесс программирования. Как и многие важные открытия, эта новая парадигма в основном довольно проста, но имеет далеко идущие последствия.

Упоминание о новой парадигме заставляет сразу же подумать о другой новой парадигме, популярность которой неуклонно растет, а именно об объектно-ориентированном программировании (обычно сокращенно ООП). То, что я собираюсь описать, не является ООП, но имеет определенное сходство с ним, и особенно с более продвинутыми концепциями ООП, в частности с концепцией «активных объектов». В конечном итоге эти две парадигмы, похоже, находятся на сходящемся пути, и, как я опишу в более поздней главе, я считаю, что вполне возможно объединить два набора концепций для достижения лучшего из обоих миров. Однако, в большей части этой книги я буду представлять наши концепции и опыт по мере их исторической эволюции, используя нашу собственную терминологию.

Проработав несколько лет в компьютерном бизнесе, я обнаружил, что не понимаю, почему программирование приложений должно быть таким сложным. Его сложность, конечно же, не является сложностью самих алгоритмов или логики. С арифметической точки зрения в бизнес-программировании редко можно встретить умножение или деление, не говоря уже о чем-то столь же загадочном, как квадратный корень.

Я и ряд других специалистов в этой области пришли к выводу, что основная причина проблемы - это, на самом деле, то же самое, что привело к самой компьютерной революции, а именно компьютерная модель фон Неймана.

Эта традиционная модель, которая была очень продуктивной в течение последних нескольких десятилетий, разработана на основе единого счетчика инструкций, который последовательно проходит через строки кодов, решая, что делать на каждом этапе. Эти коды могут обрабатываться как данные (например, компиляторами), так и как команды. Эта конструкция обычно, но не обязательно, сочетается с однородным массивом ячеек памяти, из которых инструкции берут данные и в которые они их возвращают. Как описано в недавней статье Valiant (1990), сила этой модели проистекает из того факта, что она действует как мост между двумя «разнообразными и хаотическими» мирами (как их называет Valiant) аппаратного и программного обеспечения, позволяя они должны развиваться отдельно. Но, по той же причине, успех этой модели убедил практиков, что сложности, с которыми мы сталкиваемся, не могут быть вызваны какими-либо фундаментальными проблемами с этим набором концепций.

Программисты недостаточно умны, у них недостаточно хороших инструментов, у них недостаточно математического образования или они недостаточно много работают - уверен, и вы столкнулись со всем этим. Я не верю, что хоть что-то из этого правда - существует гораздо более фундаментальная проблема, а именно, что на базовом уровне среда просто не подходит для поставленной задачи. Представьте на секунду, как построить из глины автомобиль! Он очень пластичен во влажном состоянии, вы сможете сделать что угодно, но после обжига он очень твердый, но очень хрупкий! Именно так "ощущается" большинство наших приложений сегодня!

Настало время для новой парадигмы, что заменит модель фон Неймана в качестве моста между аппаратным и программным обеспечением. Та, что мы будем описывать, похожа предлагаемую Valiant (я расскажу о нем подробно в главе 27), и на самом деле кажется одной из семейства связанных концепций, появившихся за последние несколько лет. Общая концепция, лежащая в основе большей части этой работы, состоит в том, что для решения этих проблем мы должны
перестать беспокоится о порядке исполнения инструкций, как мы делаем это при Фон-Неймановском программировании и начать и структурировать программы как совокупности взаимодействующих асинхронных процессов. Если вы посмотрите на приложения, большие, чем одна программа, или загляните внутрь машины, вы обнаружите, что многие процессы идут параллельно. Только в рамках одной программы (шага задачи или транзакции) вы по-прежнему обнаруживаете строгую традиционную последовательную логику. Мы думали, жесткий контроль последовательности выполнения - единственный способ получить предсказуемый код и, следовательно, он необходим для надежных систем. Оказывается, машины (и люди) работают эффективнее, если оставить лишь те ограничения, которые имеют значение, и убрать остальные, и вы можете сделать это без потери надежности. Цель этой книги - попытаться описать совокупность опыта, накопленного с использованием определенного набора реализаций этой концепции на протяжении многих лет, поэтому я не буду вдаваться в подробности на данном этапе. В этой главе мы будем больше говорить об истории этой концепции, чем о конкретных реализациях или опыте, полученном при их использовании.

Еще один фактор, который заставляет меня думать, что об этой технологии пора обнародовать, - мы сталкиваемся с растущим кризисом в разработке приложений.

Одновременно с появлением новых требований технологии меняется все быстрее. Набор концепций, что я буду описывать, кажется, хорошо согласуется с текущими направлениями как для программного, так и для аппаратного обеспечения. Он не только может естественным образом поддерживать требования распределенных, гетерогенных приложений, но также кажется подходящей технологией для новых многопроцессорных машин, над которыми работают университеты и ведущие производители компьютеров по всему миру. Как покойный Уэйн Стивенс, известный писатель о методологиях разработки приложений, отмечал в нескольких своих статьях (например, Stevens 1985), парадигма, которую мы будем описывать, обеспечивает последовательный, естественный взгляд на приложения с точки зрения их работы. Поскольку вы можете описывать ручные приложения с помощью диаграмм потоков данных, связь между ручными и системными процедурами может быть легко показана.

В дальнейшем я буду использовать термин «Flow-Based programming» (или сокращенно FBP) для описания этого нового набора концепций и программного обеспечения, необходимого для его поддержки. В прошлом мы использовали термин «data-flow», поскольку он передает ряд наиболее важных аспектов этой технологии, но существует значительный объем опубликованных работ по так называемым «data-flow архитектурам» в компьютерном дизайне и связанном с ними программном обеспечении. (Например, очень интересная работа из MIT), поэтому термин «data-flow» может вызвать путаницу в некоторых академических кругах. Несколько лет назад мне также указали, что, когда поток управления требуется явно, FBP может предоставить его с помощью таких механизмов, как триггеры, поэтому термин «Потоко-Ориентированное программирование» избегает коннотации, что мы не можем управлять потоком исполнения. Это не означает, что у FBP и data-flow мало общего - компьютерные архитектуры потоков данных возникают также из понимания, что конструкция машины фон Неймана, бывшая столь успешной в прошлом, должна быть обобщена, если мы хотим двигаться дальше.

---
Одно существенное различие между двумя школами, по крайней мере, сейчас, заключается в том, что большая часть других работ с потоками данных была математически ориентирована, поэтому она имеет тенденцию работать с числами и массивами чисел. Хотя моя ранняя работа с потоками данных в конце 60-х годов также включала простые числовые значения, текущих через сеть функциональных блоков, мой опыт работы с системами моделирования привел меня к осознанию того, что в бизнес-приложениях было бы более продуктивно иметь структурированные потоки данных. Объекты, которые я назвал «сущностями». Это название отражало, что эти структурированные объекты имеют тенденцию представлять сущности во внешнем мире. (В нашей более поздней работе мы поняли, что имя «сущность» может вызвать путаницу с идеей сущностей в моделировании данных, хотя есть точки сходства, поэтому мы решили использовать другое слово . Такая система - естественный способ для моделирования приложений и различие между моделированием и реализацией становится гораздо менее значительным, чем в традиционном программировании. Вы можете думать о "сущности" как о записи в хранилище, но как записи активной (в том смысле, что она запускает события), а не как о пассивной (просто читается или записывается). Сущности проходят через сеть процессов, например автомобили в городе или лодки в речной системе. Они отличаются от математических токенов компьютеров с потоком данных или моих ранних работ главным образом тем, что они имеют структуру: каждая сущность представляет собой объект с атрибутами, например, у сотрудника будут такие атрибуты, как зарплата, дата найма, менеджер и т. Д. Как вы читаете В этой книге должно стать ясно, почему должен быть хотя бы один уровень приложения, на котором объекты перемещаются как отдельные единицы, хотя вполне может быть возможно интегрировать различные подходы к потокам данных на более низких уровнях. 

At this point I am going to have to describe FBP briefly, to give the reader something to visualize, but first a caveat: the brief description that follows will probably not be enough to let you picture what FBP is and how it does it. If we don't do this at this point, however, experience shows that readers find it hard to relate what I am describing to their own knowledge. The reverse risk is that they may jump to conclusions which may prevent them from seeing what is truly new about the concepts I will be describing later. I call this the "It's just..." syndrome.

In conventional programming, when you sit down to write a program, you write code down the page - a linear string of statements describing the series of actions you want the computer to execute. Since we are of course all writing structured code now, we start with a main line containing mostly subroutine calls, which can then be given "meaning" later by coding up the named subroutines. A number of people have speculated about the possibility of instead building a program by just plugging prewritten pieces of logic together. This has sometimes been called 'Legoland' programming. Even though that is essentially what we do when we use utilities, there has always been some doubt whether this approach has the power to construct large scale applications, and, if it has, whether such applications would perform. I now have the pleasure to announce that the answer is 'Yes' to both these questions!

The "glue" that FBP uses to connect the pieces together is an example of what Yale's Gelernter and Carriero (1992) have called a "coordination language". I feel the distinction between coordination languages and procedural languages is a useful one, and helps to clarify what is different about FBP. Conventional programming languages instruct the machine what logic to execute; coordination languages tell the machine how to coordinate multiple modules written in one or several programming languages. There is quite a bit of published material on various approaches to coordination, but much of that work involves the use of special-purpose languages, which reduces the applicability of these concepts to traditional languages and environments. Along with Gelernter and Carriero, I feel a better approach is to have a language-independent coordination notation, which can coordinate modules written in a variety of different procedural languages. The individual modules have to have a common Application Programming Interface to let them talk to the coordination software, but this can be relatively simple.

Coordination and modularity are two sides of the same coin, and several years ago Nate Edwards of IBM coined the term "configurable modularity" to denote an ability to reuse independent components just by changing their interconnections, which in his view characterizes all successful reuse systems, and indeed all systems which can be described as "engineered". Although I am not sure when Nate first brought the two words "configurable" and "modularity" together, the report on a planning session in Palo Alto in 1976 uses the term, and Nate's 1977 paper (Edwards 1977) contains both the terms "configurable architecture" and "controlled modularity". While Nate Edwards' work is fairly non-technical and pragmatic, his background is mainly in hardware, rather than software, which may be why his work has not received the attention it deserves. One of the important characteristics of a system exhibiting configurable modularity, such as most modern hardware or Flow-Based Programming, is that you can build systems out of "black box" reusable modules, much like the chips which are used to build logic in hardware. You also, of course, have to have something to connect them together with, but they do not have to be modified in any way to make this happen. Of course, this is characteristic of almost all the things we attach to each other in real life - in fact, almost everywhere except in conventional programming. In FBP, these black boxes are the basic building blocks that a developer uses to build an application. New black boxes can be written as needed, but a developer tries to use what is available first, before creating new components. In FBP, the emphasis shifts from building everything new to connecting preexisting pieces and only building new when building a new component is cost-justified. Nate Edwards played a key role in getting the hardware people to follow this same principle - and now of course, like all great discoveries, it seems that we have always known this! We have to help software developers to move through the same paradigm shift. If you look at the literature of programming from this standpoint, you will be amazed at how few writers write from the basis of reuse - in fact the very term seems to suggest an element of surprise, as if reuse were a fortuitous occurrence that happens seldom and usually by accident! In real life, we use a knife or a fork - we don't reuse it!

We will be describing similarities between FBP and other similar pieces of software in later chapters, but perhaps it would be useful at this point to use DOS pipes to draw a simple analogy. If you have used DOS you will know that you can take separate programs and combine them using a vertical bar (|), e.g.

A | B

This is a very simple form of what I have been calling coordination of separate programs. It tells the system that you want to feed the output of A into the input of B, but neither A nor B have to be modified to make this happen. A and B have to have connection points ("plugs" and "sockets") which the system can use, and of course there has to be some software which understands the vertical bar notation and knows what to do with it. FBP broadens this concept in a number of directions which vastly increase its power. It turns out that this generalization results in a very different approach to building applications, which results in systems which are both more reliable and more maintainable. In the following pages I hope to be able to prove this to your satisfaction!

The FBP systems which have been built over the last 20 years have therefore basically all had the following components:

    a number of precoded, pretested functions, provided in object code form, not source code form ("black boxes") - this set is open-ended and (hopefully) constantly growing

    a "Driver" - a piece of software which coordinates the different independent modules, and implements the API (Application Programming Interface) which is used by the components to communicate with each other and with the Driver

    a notation for specifying how the components are connected together into one or more networks (an FBP application designer starts with pictures, and then converts them into specifications to be executed by the Driver)

    this notation can be put into a file for execution by the Driver software. In the most successful implementation of FBP so far (DFDM - described in the next section of this chapter), the network could either be compiled and link edited to produce an executable program, or it could be interpreted directly (with of course greater initialization overhead). In the interpreted mode, the components are loaded in dynamically, so you can make changes and see the results many times in a few minutes. As we said before, people find this mode extremely productive. Later, when debugging is finished, you can convert the interpretable form to the compilable form to provide better performance for your production version.

    procedures to enable you to convert, compile and package individual modules and partial networks

    documentation (reference and tutorial) for all of the above

In the above list I have not included education - but of course this is probably the most important item of all. To get the user started, there is a need for formal education - this may only take a few days or weeks, and I hope that this book will get the reader started on understanding many of the basic concepts. However, education also includes the practical experience that comes from working with many different applications, over a number of months or years. In this area especially, we have found that FBP feels very different from conventional programming. Unlike most other professions, in programming we tend to underestimate the value of experience, which may in fact be due to the nature of the present-day programming medium. In other professions we do not recommend giving a new practitioner a pile of books, and then telling him or her to go out and do brain surgery, build a bridge, mine gold or sail across the Atlantic. Instead it is expected that there will be a series of progressive steps from student or apprentice to master. Application development using FBP feels much more like an engineering-style discipline: we are mostly assembling structures out of preexisting components with well-defined specifications, rather than building things from scratch using basic raw materials. In such a medium, experience is key: it takes time to learn what components are available, how they fit together and what trade-offs can be made. However, unlike bridge-builders, application developers using FBP can also get simple applications working very fast, so they can have the satisfaction of seeing quite simple programs do non-trivial things very early. Education in FBP is a hands-on affair, and it is really a pleasure seeing people's reactions when they get something working without having to write a line of code!

Now that graphics hardware and software have become available at reasonable cost and performance, it seems very desirable to have graphical front-ends for our FBP systems. Since FBP is a highly visual notation, we believe that a graphical front-end will make it even more usable. Some prototype work has already been done along these lines and seems to bear this idea out. Many potential users of FBP systems will already have one or more graphical design tools, and, as we shall see, there is an especially good match between Structured Analysis and FBP, so that it seems feasible, and desirable, to base FBP graphical tools on existing graphical tools for doing Structured Analysis, with the appropriate information added for creating running FBP programs.

Now I feel it would be useful to give you a bit of historical background on FBP: the first implementation of this concept was built by myself in 1969 and 1970 in Montreal, Quebec. This proved very productive - so much so that it was taken into a major Canadian company, where it was used for all the batch programming of a major on-line system. This system was called the Advanced Modular Processing System (AMPS). This system and the experience gained from it are described in a fair amount of detail in an article I wrote a few years later for the IBM Systems Journal (Morrison 1978). I am told this was the first article ever published in the Systems Journal by an author from what was then called the Americas/Far East area of IBM (comprising Canada, South America and the Far East).

Although the concepts are not well known, they have actually been in the public domain for many years. The way this happened is as follows: in late 1970 or early '71 I approached IBM Canada's Intellectual Property department to see if we could take out a patent on the basic idea. Their recommendation, which I feel was prescient, was that this concept seemed to them more like a law of nature, which is not patentable. They did recommend, however, that I write up a Technical Disclosure Bulletin (TDB), which was duly published and distributed to patent offices world-wide (Morrison 1971). A TDB is a sort of inverse patent - while a patent protects the owner but requires him or her to try to predict all possible variations on a concept, a TDB puts a concept into the public domain, and thereby protects the registering body from being restricted or impeded in the future in any use they may wish to make of the concept. In the case of a TDB, it places the onus on someone else who might be trying to patent something based on your concept to prove that their variation was not obvious to someone "skilled in the art".

Towards the end of the 80's, Wayne Stevens and I jointly developed a new version of this software, called the Data Flow Development Manager (DFDM). It is described in Appendix A of Wayne Stevens' latest book (Stevens 1991) (which, by the way, contains a lot of good material on application design techniques in general). What I usually refer to in what follows as "processes" were called "coroutines" in DFDM, after Conway (1963), who described an early form of this concept in a paper back in the 60's, and foresaw even then some of its potential. "Coroutine" is formed from the word "routine" together with the Latin prefix meaning "with", as compared with "subroutine", which is formed with the prefix meaning "under". (Think of "cooperative" vs. "subordinate").

DFDM was used for a number of projects (between 40 and 50) of various sizes within IBM Canada. A few years later, Kenji Terao got a project started within IBM Japan to support us in developing an improved version for the Japanese market. This version is, at the time of writing, the only dialect of FBP which has been made available in the market-place, and I believe enormous credit is due to Kenji and all the dedicated and forward-looking people in IBM Japan who helped to make this happen. While this version of DFDM was in many ways more robust or "industrial strength" than the one which we had been using within IBM Canada, much of the experience which I will be describing in the following pages is based on what we learned using the IBM Canada internal version of DFDM, or on the still earlier AMPS system. Perhaps someone will write a sequel to this book describing the Japanese experience with DFDM...

Last, but I hope not least, there is a PC-based system written in C, which attempts to embody many of the best ideas of its ancestors. [Reference to HOMEDATA in book removed from this web page, as they are (to the best of my knowledge) no longer involved in this effort.] It [THREADS] has been available since the summer of 1993, running on Intel-based machines. It has been tested on 268, 386 and 486-based machines. Since it is written in C, we are hoping that it will also be possible to port it later to other versions of C, although there is a small amount of environment-dependent code which will have to be modified by hand. This software is called THREADS - THREads-based Application Development System (I love self-referential names!) [see THREADS]. Like DFDM, it also has interpreted and compiled versions, so applications can be developed iteratively, and then compiled to produce a single EXE file, which eliminates the network decoding phase.

The terminology used in this book is not exactly the same as that used by AMPS and DFDM, as a number of these terms turned out to cause confusion. For instance, the data chunks that travel between the asynchronous processes were called "entities" in AMPS and DFDM, but, as I said above, this caused confusion for people experienced in data modelling. They do seem to correspond with the "entities" of data modelling, but "entities" have other connotations which could be misleading. "Objects" would present other problems, and we were not comfortable with the idea of creating totally new words (although some writers have used them effectively). The "tuples" of Carriero and Gelernter's Linda (1989) are very close, but this name also presents a slightly different image from the FBP concept. We therefore decided to use the rather neutral term "information packet" (or "IP" for short) for this concept. This term was coined as part of work that we did following the development of DFDM, in which we also tied FBP concepts in with other work appearing in the literature or being developed in other parts of IBM. Some of the extensions to the basic AMPS and DFDM substructure that I will be talking about later were also articulated during this period. When I need to refer to ideas drawn from this work I will use the name FPE (for Flow-Based Programming Environment), although that is not the acronym used by that project. THREADS follows this revised terminology, and includes a number of ideas from FPE.

As I stated in the prologue, for most of my 33 years in the computer business I have been almost exclusively involved with business applications. Although business applications are often more complex than scientific applications, the academic community generally has not shown much interest in this area up until now. This is a "catch 22" situation, as business would benefit from the work done in academia, yet academia (with some noteworthy exceptions) tends not to regard business programming as an interesting area to work in. My hope is that FBP can act as a bridge between these two worlds, and in later chapters I will be attempting to tie FBP to other related theoretical work which working programmers probably wouldn't normally encounter. My reading in the field suggests that FBP has sound theoretical foundations, and yet it can perform well enough that you can run a company on it, and it is accessible to trainee programmers (sometimes more easily than for experienced ones!). AMPS has been in use for 20 years, supporting one of the biggest companies in North America, and as recently as this year (1992), one of their senior people told me, "AMPS has served us well, and we expect it will continue to do so for a long time to come." Business systems have to evolve over time as the market requirements change, so clearly their system has been able to grow and adapt over the years as the need arose - this is a living system, not some outdated curiosity which has become obsolete with the advance of technology.

And now I would like to conclude this chapter with an unsolicited testimonial from a DFDM user, which we received a few years ago:

    "I have a requirement to merge 23 ... reports into one .... As all reports are of different length and block size this is more difficult in a conventional PLI environment. It would have required 1 day of work to write the program and 1 day to test it. Such a program would use repetitive code. While drinking coffee 1 morning I wrote a DFDM network to do this. It was complete before the coffee went cold [my italics]. Due to the length of time from training to programming it took 1 day to compile the code. Had it not been for the learning curve it could have been done in 5 minutes. During testing a small error was found which took 10 minutes to correct. As 3 off-the-shelf coroutines were used, PLI was not required. 2 co-routines were used once, and 1 was used 23 times. Had it not been for DFDM, I would have told the user that his requirement was not cost justified. It took more time to write this note than the DFDM network."

Notice that in his note, Rej (short for Réjean), who, by the way, is a visually impaired application developer with many years of experience in business applications, mentioned all the points that were significant to him as a developer - he zeroed right in on the amount of reuse he was getting, because functions he could get right off the shelf were ones he didn't have to write, test and eventually maintain! In DFDM, "coroutines" are the basic building blocks, which programmers can hook together to build applications. They are either already available ("on the shelf"), or the programmer can write new ones, in which case he or she will naturally try to reuse them as often as possible - to get the most bang for the proverbial buck. Although it is not very hard to write new PL/I coroutines, the majority of application developers don't want to write new code - they just want to get their applications working for the client, preferably using as little programming effort as will suffice to get a quality job done. Of course there are always programmers who love the process of programming and, as we shall see in the following pages, there is an important role for them also in this new world which is evolving.

Rej's note was especially satisfying to us because he uses special equipment which converts whatever is on his screen into spoken words. Since FBP has always seemed to me a highly visual technique, I had worried about whether visually impaired programmers would have any trouble using it, and it was very reassuring to find that Rej was able to make such productive use of this technology. In later discussions with him, he has stressed the need to keep application structures simple. In FBP, you can use hierarchic decomposition to create multiple layers, each containing a simple structure, rather than being required to create a single, flat, highly complex structure. In fact, structures which are so complex that he would have trouble with them are difficult for everyone. He also points out that tools which he would find useful, such as something which can turn network diagrams into lists of connections, would also significantly assist normally sighted people as they work with these structures.

Rej's point about the advantages of keeping the structures simple is also borne out by the fact that another application of DFDM resulted in a structure of about 200 processes, but the programmer involved (another very bright individual) never drew a single picture! He built it up gradually using hierarchical decomposition, and it has since had one of the lowest error rates of any application in the shop. I hope that, as you read on, you will be able to figure out some of the reasons for this high level of reliability for yourself.

In what follows, I will be describing the main features of FBP and what we have learned from developing and using its various implementations. Information on some of these has appeared in a number of places and I feel it is time to try to pull together some of the results of 20 years of experience with these concepts into a single place, so that the ideas can be seen in context. A vast number of papers have appeared over the years, written by different writers in different fields of computing, which I believe are all various facets of a single diamond, but I hope that, by pulling a lot of connected ideas together in one place, the reader will start to get a glimpse of the total picture. Perhaps there is someone out there who is waiting for these ideas, and will be inspired to carry them further, either in research or in the marketplace!
